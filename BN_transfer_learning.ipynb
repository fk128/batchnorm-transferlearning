{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/data/datasets'\n",
    "OUTPUT_PATH = '/data/tlbn/output'\n",
    "MAX_EPOCHS = 500\n",
    "REPETITIONS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import shutil\n",
    "from functools import partial\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chexpert "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_chexpert(**kwargs):\n",
    "    \"\"\"\n",
    "    Download chexport-small from https://stanfordmlgroup.github.io/competitions/chexpert/\n",
    "    \"\"\"\n",
    "    class params:\n",
    "        batch_size = 32\n",
    "        input_shape = (224, 224, 3)\n",
    "        labels = ['No Finding', 'Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Pleural Effusion']\n",
    "        mode = 'multi-binary'\n",
    "        dropout = 0\n",
    "        loss = 'binary_crossentropy'\n",
    "        flipping = \"horizontal\"\n",
    "\n",
    "    labels = params.labels\n",
    "    df = pd.read_csv(f'{DATA_PATH}/chexpert/downloads/manual/CheXpert-v1.0-small/train.csv', index_col=0)\n",
    "    all_labels = df.columns[4:]\n",
    "    # drop lateral\n",
    "    df = df[df['Frontal/Lateral'] == 'Frontal']\n",
    "\n",
    "    # drop uncertain\n",
    "    for label in labels:\n",
    "        df = df[(df[label] != -1)]\n",
    "    df = df[labels]\n",
    "    df['Patient'] = df.index\n",
    "    df['Patient'] = df['Patient'].apply(lambda x: x.split('/')[2])\n",
    "    patient_ids = pd.Series(df['Patient'].unique())\n",
    "    test_patients = patient_ids.sample(frac=0.3, random_state=0)\n",
    "    train_patients = patient_ids.drop(test_patients.index)\n",
    "    val_patients = train_patients.sample(frac=0.3, random_state=0)\n",
    "    train_patients = train_patients.drop(val_patients.index)\n",
    "    train = df[df['Patient'].isin(train_patients)]\n",
    "    val = df[df['Patient'].isin(train_patients)]\n",
    "    test = df[df['Patient'].isin(test_patients)]\n",
    "\n",
    "    def subsample(df, labels, num_per_label=1000):\n",
    "        data = []\n",
    "        for i, label in enumerate(labels):\n",
    "            data.append(df[df[label] == 1].sample(num_per_label, random_state=i))\n",
    "        data.append(df[df['No Finding'] == 1].sample(num_per_label, random_state=i))\n",
    "        return pd.concat(data).drop_duplicates()\n",
    "\n",
    "    test = subsample(test, labels, num_per_label=200)\n",
    "    train = subsample(train, labels, num_per_label=2000)\n",
    "    val = subsample(val, labels, num_per_label=150)\n",
    "\n",
    "    params.labels = params.labels[1:]\n",
    "    train_gen = tf.keras.preprocessing.image.ImageDataGenerator(horizontal_flip=True)\n",
    "    train_ds = train_gen.flow_from_dataframe(train.reset_index(),\n",
    "                                             directory=f'{DATA_PATH}/chexpert/downloads/manual/',\n",
    "                                             x_col='Path',\n",
    "                                             class_mode='raw',\n",
    "                                             y_col=params.labels,\n",
    "                                             batch_size=params.batch_size,\n",
    "                                             target_size=params.input_shape[:2]\n",
    "                                             )\n",
    "\n",
    "    val_gen = tf.keras.preprocessing.image.ImageDataGenerator()\n",
    "    val_ds = val_gen.flow_from_dataframe(val.reset_index(),\n",
    "                                         directory=f'{DATA_PATH}/chexpert/downloads/manual/',\n",
    "                                         x_col='Path',\n",
    "                                         class_mode='raw',\n",
    "                                         y_col=params.labels,\n",
    "                                         batch_size=params.batch_size,\n",
    "                                         shuffle=False,\n",
    "                                         target_size=params.input_shape[:2]\n",
    "                                         )\n",
    "\n",
    "    test_gen = tf.keras.preprocessing.image.ImageDataGenerator()\n",
    "    test_ds = test_gen.flow_from_dataframe(test.reset_index(),\n",
    "                                           directory=f'{DATA_PATH}/chexpert/downloads/manual/',\n",
    "                                           x_col='Path',\n",
    "                                           class_mode='raw',\n",
    "                                           y_col=params.labels,\n",
    "                                           batch_size=params.batch_size,\n",
    "                                           shuffle=False,\n",
    "                                           target_size=params.input_shape[:2]\n",
    "                                           )\n",
    "                                           \n",
    "    return train_ds, val_ds, test_ds, params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Camelyon 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_patch_camelyon17_download():\n",
    "    \"\"\"\n",
    "    download manually from https://wilds.stanford.edu/datasets/#camelyon17\n",
    "    \"\"\"\n",
    "    path = Path(f'{DATA_PATH}/camelyon17_v1.0')\n",
    "    df = pd.read_csv(f'{str(path)}/metadata.csv', index_col=0, dtype={'patient': 'str'})\n",
    "    (path / 'splits').mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    for hosp_idx in range(5):\n",
    "        hosp_path = path / 'splits' / f'hosp_{hosp_idx}'\n",
    "        hosp_path.mkdir(parents=True, exist_ok=True)\n",
    "        for l in {0, 1}:\n",
    "            subset_path = hosp_path / str(l)\n",
    "            subset_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for patient, node, x, y, h, label in df.loc[:,\n",
    "                                         ['patient', 'node', 'x_coord', 'y_coord', 'center', 'tumor']].itertuples(\n",
    "        index=False, name=None):\n",
    "        src = path / Path(f'patches/patient_{patient}_node_{node}/patch_patient_{patient}_node_{node}_x_{x}_y_{y}.png')\n",
    "        dest = path / Path(f'splits/hosp_{h}/{label}/patch_patient_{patient}_node_{node}_x_{x}_y_{y}.png')\n",
    "        try:\n",
    "            shutil.move(src, dest)\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_patch_camelyon17(mode='full'):\n",
    "    \"\"\"\n",
    "    Hosp 0,3,4: original training, validation, IID\n",
    "    Hosp 2: original test set\n",
    "    Hosp 1: validation OOD\n",
    "    use only hosp 4 as training, hosp 1 as validation, and then\n",
    "    the rest as 3 separate test sets\n",
    "    \"\"\"\n",
    "    class params:\n",
    "        batch_size = 64\n",
    "        input_shape = (96, 96, 3)\n",
    "        num_classes = 2\n",
    "        mode = 'binary'\n",
    "        dropout = 0.5\n",
    "        labels = ['0', '1']\n",
    "        loss = tf.keras.losses.binary_crossentropy\n",
    "        flipping = \"horizontal_and_vertical\"\n",
    "\n",
    "    root_path = Path(f'{DATA_PATH}/camelyon17_v1.0/splits')\n",
    "    train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        root_path / 'hosp_4',\n",
    "        seed=1337,\n",
    "        image_size=params.input_shape[:2],\n",
    "        batch_size=params.batch_size,\n",
    "    )\n",
    "    val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        root_path / 'hosp_1',\n",
    "        seed=1338,\n",
    "        image_size=params.input_shape[:2],\n",
    "        batch_size=params.batch_size,\n",
    "        validation_split=0.5,\n",
    "        subset='validation'\n",
    "    )\n",
    "\n",
    "    if mode == 'small':\n",
    "        train_ds = train_ds.take(100)\n",
    "        val_ds = val_ds.take(15)\n",
    "    test_ds = {}\n",
    "    for name in ['hosp_2', 'hosp_3', 'hosp_0']:\n",
    "        test_ds[name] = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "            root_path / name,\n",
    "            seed=1339,\n",
    "            image_size=params.input_shape[:2],\n",
    "            batch_size=params.batch_size,\n",
    "        )\n",
    "        \n",
    "    return train_ds, val_ds, test_ds, params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Camelyon 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_patch_camelyon16(mode='full'):\n",
    "    \"\"\"\n",
    "    https://www.tensorflow.org/datasets/catalog/patch_camelyon\n",
    "    \"\"\"\n",
    "    class params:\n",
    "        batch_size = 64\n",
    "        input_shape = (96, 96, 3)\n",
    "        num_classes = 2\n",
    "        mode = 'binary'\n",
    "        dropout = 0.5\n",
    "        labels = ['0', '1']\n",
    "        loss = 'binary_crossentropy'\n",
    "        flipping = \"horizontal_and_vertical\"\n",
    "\n",
    "    if mode == 'full':\n",
    "        train_ds, ds_info = tfds.load('patch_camelyon', split='train',\n",
    "                                      shuffle_files=True, with_info=True,\n",
    "                                      data_dir=DATA_PATH,\n",
    "                                      read_config=tfds.ReadConfig(shuffle_seed=0,\n",
    "                                                                  shuffle_reshuffle_each_iteration=True))\n",
    "        val_ds, ds_info = tfds.load('patch_camelyon', split='validation',\n",
    "                                    shuffle_files=True, with_info=True,\n",
    "                                    data_dir=DATA_PATH,\n",
    "                                    read_config=tfds.ReadConfig(shuffle_seed=0, shuffle_reshuffle_each_iteration=True))\n",
    "    elif mode == 'small':\n",
    "        train_ds, ds_info = tfds.load('patch_camelyon', split='train[:5000]',\n",
    "                                      shuffle_files=True, with_info=True,\n",
    "                                      data_dir=DATA_PATH,\n",
    "                                      read_config=tfds.ReadConfig(shuffle_seed=0,\n",
    "                                                                  shuffle_reshuffle_each_iteration=True))\n",
    "        val_ds, ds_info = tfds.load('patch_camelyon', split='validation[:1000]',\n",
    "                                    shuffle_files=True, with_info=True,\n",
    "                                    data_dir=DATA_PATH,\n",
    "                                    read_config=tfds.ReadConfig(shuffle_seed=0, shuffle_reshuffle_each_iteration=True))\n",
    "    test_ds, ds_info = tfds.load('patch_camelyon', split='test',\n",
    "                                 shuffle_files=True, with_info=True,\n",
    "                                 data_dir=DATA_PATH)\n",
    "\n",
    "    print(f\"total of training samples: {tf.data.experimental.cardinality(train_ds)}\")\n",
    "    print(f\"total of validation samples: {tf.data.experimental.cardinality(val_ds)}\")\n",
    "    print(f\"total of test samples: {tf.data.experimental.cardinality(test_ds)}\")\n",
    "    tfds.visualization.show_examples(train_ds, ds_info)\n",
    "\n",
    "    def load_image(image):\n",
    "        return image['image'], image['label']\n",
    "    train_ds = train_ds.map(load_image, num_parallel_calls=tf.data.AUTOTUNE).batch(params.batch_size).prefetch(\n",
    "        tf.data.AUTOTUNE)\n",
    "    val_ds = val_ds.map(load_image, num_parallel_calls=tf.data.AUTOTUNE).batch(params.batch_size).prefetch(\n",
    "        tf.data.AUTOTUNE)\n",
    "    test_ds = test_ds.map(load_image, num_parallel_calls=tf.data.AUTOTUNE).batch(params.batch_size).prefetch(\n",
    "        tf.data.AUTOTUNE)\n",
    "        \n",
    "    print(f\"Number of training batches: {tf.data.experimental.cardinality(train_ds)}\")\n",
    "    print(f\"Number of validation batches: {tf.data.experimental.cardinality(val_ds)}\")\n",
    "    print(f\"Number of test batches: {tf.data.experimental.cardinality(test_ds)}\")\n",
    "    return train_ds, val_ds, test_ds, params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chest x-ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_chestxray(**kwargs):\n",
    "    \"\"\"\n",
    "    https://www.cell.com/cell/fulltext/S0092-8674(18)30154-5\n",
    "    TF records from https://keras.io/examples/vision/xray_classification_with_tpus/\n",
    "    \"\"\"\n",
    "    class params:\n",
    "        batch_size = 64\n",
    "        input_shape = (180, 180, 3)\n",
    "        num_classes = 2\n",
    "        mode = 'binary'\n",
    "        dropout = 0.5\n",
    "        loss = 'binary_crossentropy'\n",
    "        flipping = \"horizontal\"\n",
    "        labels = [\"NORMAL\", \"PNEUMONIA\"]\n",
    "\n",
    "    AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "    train_images = tf.data.TFRecordDataset(\n",
    "        f'{DATA_PATH}/ChestXRay2017/train/images.tfrec'\n",
    "    )\n",
    "    train_paths = tf.data.TFRecordDataset(\n",
    "        f'{DATA_PATH}/ChestXRay2017/train/paths.tfrec'\n",
    "    )\n",
    "    ds = tf.data.Dataset.zip((train_images, train_paths))\n",
    "    COUNT_NORMAL = len(\n",
    "        [\n",
    "            filename\n",
    "            for filename in train_paths\n",
    "            if \"NORMAL\" in filename.numpy().decode(\"utf-8\")\n",
    "        ]\n",
    "    )\n",
    "    print(\"Normal images count in training set: \" + str(COUNT_NORMAL))\n",
    "    COUNT_PNEUMONIA = len(\n",
    "        [\n",
    "            filename\n",
    "            for filename in train_paths\n",
    "            if \"PNEUMONIA\" in filename.numpy().decode(\"utf-8\")\n",
    "        ]\n",
    "    )\n",
    "    print(\"Pneumonia images count in training set: \" + str(COUNT_PNEUMONIA))\n",
    "    def get_label(file_path):\n",
    "        # convert the path to a list of path components\n",
    "        parts = tf.strings.split(file_path, \"/\")\n",
    "        # The second to last is the class-directory\n",
    "        return parts[-2] == \"PNEUMONIA\"\n",
    "\n",
    "    def decode_img(img):\n",
    "        # convert the compressed string to a 3D uint8 tensor\n",
    "        img = tf.image.decode_jpeg(img, channels=3)\n",
    "\n",
    "        # resize the image to the desired size.\n",
    "        return tf.image.resize(img, params.input_shape[:2])\n",
    "\n",
    "    def process_path(image, path):\n",
    "        label = get_label(path)\n",
    "        # load the raw data from the file as a string\n",
    "        img = decode_img(image)\n",
    "        return img, tf.cast(label, tf.float32)\n",
    "\n",
    "    def prepare(ds, cache=False):\n",
    "        # This is a small dataset, only load it once, and keep it in memory.\n",
    "        # use `.cache(filename)` to cache preprocessing work for datasets that don't\n",
    "        # fit in memory.\n",
    "        if cache:\n",
    "            if isinstance(cache, str):\n",
    "                ds = ds.cache(cache)\n",
    "            else:\n",
    "                ds = ds.cache()\n",
    "\n",
    "        # `prefetch` lets the dataset fetch batches in the background while the model\n",
    "        # is training.\n",
    "        ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "        return ds\n",
    "\n",
    "    def show_batch(image_batch, label_batch):\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        for n in range(25):\n",
    "            ax = plt.subplot(5, 5, n + 1)\n",
    "            plt.imshow(image_batch[n] / 255)\n",
    "            if label_batch[n]:\n",
    "                plt.title(\"PNEUMONIA\")\n",
    "            else:\n",
    "                plt.title(\"NORMAL\")\n",
    "            plt.axis(\"off\")\n",
    "\n",
    "    ds = ds.map(process_path, num_parallel_calls=AUTOTUNE)\n",
    "    ds = ds.shuffle(10000, reshuffle_each_iteration=True, seed=0)\n",
    "    ds = ds.batch(params.batch_size)\n",
    "    train_ds = ds\n",
    "    val_ds = prepare(train_ds.take(5))\n",
    "    train_ds = prepare(train_ds.skip(5))\n",
    "\n",
    "    #\n",
    "    test_images = tf.data.TFRecordDataset(\n",
    "        f'{DATA_PATH}/ChestXRay2017/test/images.tfrec'\n",
    "    )\n",
    "    test_paths = tf.data.TFRecordDataset(\n",
    "        f'{DATA_PATH}/ChestXRay2017/test/paths.tfrec'\n",
    "    )\n",
    "    test_ds = tf.data.Dataset.zip((test_images, test_paths))\n",
    "    test_ds = test_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n",
    "    test_ds = test_ds.batch(params.batch_size)\n",
    "\n",
    "    print(f\"Number of training batches: {tf.data.experimental.cardinality(train_ds)}\")\n",
    "    print(f\"Number of validation batches: {tf.data.experimental.cardinality(val_ds)}\")\n",
    "    print(f\"Number of test batches: {tf.data.experimental.cardinality(test_ds)}\")\n",
    "    \n",
    "    return train_ds, val_ds, test_ds, params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Malaria<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_malaria(**kwargs):\n",
    "    \"\"\"\n",
    "    available from tensorflow datasets\n",
    "    https://www.tensorflow.org/datasets/catalog/malaria\n",
    "    \"\"\"\n",
    "    (train_ds, val_ds, test_ds), ds_info = tfds.load('malaria',\n",
    "                                                     split=['train[:70%]',\n",
    "                                                            'train[70%:85%]',\n",
    "                                                            'train[85%:]'],\n",
    "                                                     shuffle_files=True,\n",
    "                                                     with_info=True,\n",
    "                                                     read_config=tfds.ReadConfig(shuffle_seed=0,\n",
    "                                                                                 shuffle_reshuffle_each_iteration=True)\n",
    "                                                     )\n",
    "\n",
    "    class params:\n",
    "        input_shape = (120, 120, 3)\n",
    "        batch_size = 32\n",
    "        num_classes = 2\n",
    "        mode = 'binary'\n",
    "        dropout = 0.5\n",
    "        labels = [i for i in range(num_classes)]\n",
    "        loss = 'binary_crossentropy'\n",
    "        flipping = \"horizontal_and_vertical\"\n",
    "\n",
    "    tfds.visualization.show_examples(train_ds, ds_info)\n",
    "    print(f\"total of training samples: {tf.data.experimental.cardinality(train_ds)}\")\n",
    "    print(f\"total of validation samples: {tf.data.experimental.cardinality(val_ds)}\")\n",
    "    print(f\"total of test samples: {tf.data.experimental.cardinality(test_ds)}\")\n",
    "    \n",
    "    def load_image(image):\n",
    "        return tf.image.resize(image['image'], params.input_shape[:2]), image['label']\n",
    "\n",
    "    train_ds = train_ds.map(load_image).batch(params.batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    val_ds = val_ds.map(load_image).batch(params.batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    test_ds = test_ds.map(load_image).batch(params.batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    print(f\"Number of training batches: {tf.data.experimental.cardinality(train_ds)}\")\n",
    "    print(f\"Number of validation batches: {tf.data.experimental.cardinality(val_ds)}\")\n",
    "    print(f\"Number of test batches: {tf.data.experimental.cardinality(test_ds)}\")\n",
    "    return train_ds, val_ds, test_ds, params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colorectal histology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_colorectal_histology(**kwargs):\n",
    "    \"\"\"\n",
    "    https://www.tensorflow.org/datasets/catalog/colorectal_histology\n",
    "    \"\"\"\n",
    "    (train_ds, val_ds, test_ds), ds_info = tfds.load('colorectal_histology',\n",
    "                                                     split=['train[:70%]',\n",
    "                                                            'train[70%:85%]',\n",
    "                                                            'train[85%:]'],\n",
    "                                                     shuffle_files=True,\n",
    "                                                     read_config=tfds.ReadConfig(shuffle_seed=0,\n",
    "                                                                                 shuffle_reshuffle_each_iteration=True),\n",
    "                                                     with_info=True)\n",
    "    class params:\n",
    "        input_shape = (150, 150, 3)\n",
    "        batch_size = 32\n",
    "        num_classes = 8\n",
    "        mode = 'categorical'\n",
    "        dropout = 0.5\n",
    "        labels = [\"tumor\",\n",
    "                  \"stroma\",\n",
    "                  \"complex\",\n",
    "                  \"lympho\",\n",
    "                  \"debris\",\n",
    "                  \"mucosa\",\n",
    "                  \"adipose\",\n",
    "                  \"empty\"]\n",
    "\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy\n",
    "        flipping = \"horizontal_and_vertical\"\n",
    "    tfds.visualization.show_examples(train_ds, ds_info)\n",
    "\n",
    "    print(f\"total of training samples: {tf.data.experimental.cardinality(train_ds)}\")\n",
    "    print(f\"total of validation samples: {tf.data.experimental.cardinality(val_ds)}\")\n",
    "    print(f\"total of test samples: {tf.data.experimental.cardinality(test_ds)}\")\n",
    "\n",
    "    def load_image(image):\n",
    "        print(image.keys())\n",
    "        return image['image'], image['label']\n",
    "\n",
    "    train_ds = train_ds.map(load_image).batch(params.batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    val_ds = val_ds.map(load_image).batch(params.batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    test_ds = test_ds.map(load_image).batch(params.batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    print(f\"Number of training batches: {tf.data.experimental.cardinality(train_ds)}\")\n",
    "    print(f\"Number of validation batches: {tf.data.experimental.cardinality(val_ds)}\")\n",
    "    print(f\"Number of test batches: {tf.data.experimental.cardinality(test_ds)}\")\n",
    "    \n",
    "    return train_ds, val_ds, test_ds, params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OCT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_oct(mode='full'):\n",
    "    \"\"\"\n",
    "    https://data.mendeley.com/datasets/rscbjbr9sj/3\n",
    "    http://dx.doi.org/10.17632/rscbjbr9sj.3\n",
    "    \"\"\"\n",
    "    class params:\n",
    "        batch_size = 32\n",
    "        input_shape = (496 // 3, 1024 // 3, 3)\n",
    "        num_classes = 4\n",
    "        mode = 'categorical'\n",
    "        dropout = 0.5\n",
    "        labels = ['0', '1', '2', '3']\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy\n",
    "        flipping = None\n",
    "\n",
    "    root_path = Path(f'{DATA_PATH}/OCT')\n",
    "    train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        root_path / 'train',\n",
    "        seed=1337,\n",
    "        image_size=params.input_shape[:2],\n",
    "        batch_size=params.batch_size,\n",
    "        validation_split=0.2,\n",
    "        subset='training'\n",
    "    )\n",
    "\n",
    "    val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        root_path / 'train',\n",
    "        seed=1337,\n",
    "        image_size=params.input_shape[:2],\n",
    "        batch_size=params.batch_size,\n",
    "        validation_split=0.2,\n",
    "        subset='validation'\n",
    "    )\n",
    "\n",
    "    if mode == 'small':\n",
    "        train_ds = train_ds.take(100)\n",
    "        val_ds = val_ds.take(15)\n",
    "    test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        root_path / 'test',\n",
    "        seed=1339,\n",
    "        image_size=params.input_shape[:2],\n",
    "        batch_size=params.batch_size,\n",
    "    )\n",
    "    \n",
    "    return train_ds, val_ds, test_ds, params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulticlassAUC(tf.keras.metrics.AUC):\n",
    "    \"\"\"AUC for a single class in a muliticlass problem.\n",
    "    Parameters\n",
    "    ----------\n",
    "    pos_label : int\n",
    "        Label of the positive class (the one whose AUC is being computed).\n",
    "    from_logits : bool, optional (default: False)\n",
    "        If True, assume predictions are not standardized to be between 0 and 1.\n",
    "        In this case, predictions will be squeezed into probabilities using the\n",
    "        softmax function.\n",
    "    sparse : bool, optional (default: True)\n",
    "        If True, ground truth labels should be encoded as integer indices in the\n",
    "        range [0, n_classes-1]. Otherwise, ground truth labels should be one-hot\n",
    "        encoded indicator vectors (with a 1 in the true label position and 0\n",
    "        elsewhere).\n",
    "    **kwargs : keyword arguments\n",
    "        Keyword arguments for tf.keras.metrics.AUC.__init__(). For example, the\n",
    "        curve type (curve='ROC' or curve='PR').\n",
    "    \"\"\"\n",
    "    def __init__(self, pos_label, from_logits=False, sparse=False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.pos_label = pos_label\n",
    "        self.from_logits = from_logits\n",
    "        self.sparse = sparse\n",
    "    def update_state(self, y_true, y_pred, **kwargs):\n",
    "        \"\"\"Accumulates confusion matrix statistics.\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_true : tf.Tensor\n",
    "            The ground truth values. Either an integer tensor of shape\n",
    "            (n_examples,) (if sparse=True) or a one-hot tensor of shape\n",
    "            (n_examples, n_classes) (if sparse=False).\n",
    "        y_pred : tf.Tensor\n",
    "            The predicted values, a tensor of shape (n_examples, n_classes).\n",
    "        **kwargs : keyword arguments\n",
    "            Extra keyword arguments for tf.keras.metrics.AUC.update_state\n",
    "            (e.g., sample_weight).\n",
    "        \"\"\"\n",
    "        if self.sparse:\n",
    "            y_true = tf.math.equal(y_true, self.pos_label)\n",
    "            y_true = tf.squeeze(y_true)\n",
    "        else:\n",
    "            y_true = y_true[..., self.pos_label]\n",
    "        if self.from_logits:\n",
    "            y_pred = tf.nn.softmax(y_pred, axis=-1)\n",
    "        y_pred = y_pred[..., self.pos_label]\n",
    "        super().update_state(y_true, y_pred, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(name, **kwargs):\n",
    "    if name == 'chexpert':\n",
    "        return load_chexpert(**kwargs)\n",
    "    elif name == 'chexpert_small':\n",
    "        return load_chexpert(**kwargs, mode='small')\n",
    "    elif name == 'chest_xray':\n",
    "        return load_chestxray(**kwargs)\n",
    "    elif name == 'patch_camelyon17':\n",
    "        return load_patch_camelyon17(**kwargs)\n",
    "    elif name == 'patch_camelyon17_small':\n",
    "        return load_patch_camelyon17(mode='small')\n",
    "    elif name == 'patch_camelyon16':\n",
    "        return load_patch_camelyon16(**kwargs)\n",
    "    elif name == 'patch_camelyon16_small':\n",
    "        return load_patch_camelyon16(**kwargs, mode='small')\n",
    "    elif name == 'colorectal_histology':\n",
    "        return load_colorectal_histology(**kwargs)\n",
    "    elif name == 'malaria':\n",
    "        return load_malaria(**kwargs)\n",
    "    elif name == 'oct_small':\n",
    "        return load_oct(mode='small')\n",
    "    else:\n",
    "        raise NotImplementedError('invalid dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape, bn_train=False, dropout=0.5, labels=('0', '1'),\n",
    "                weights='imagenet',\n",
    "                mode='binary', model_name='efficientnetb3',\n",
    "                flipping=\"horizontal_and_vertical\", do_augmentation=True):\n",
    "\n",
    "    data_augmentation = tf.keras.Sequential([\n",
    "        layers.experimental.preprocessing.RandomFlip(flipping),\n",
    "        layers.experimental.preprocessing.RandomTranslation(0.1, 0.1),\n",
    "        layers.experimental.preprocessing.RandomZoom(0.1),\n",
    "    ], name='data_augmentation')\n",
    "\n",
    "    bn_train = None if bn_train == True else bn_train\n",
    "\n",
    "    arch = {'densenet121': dict(model=tf.keras.applications.densenet.DenseNet121,\n",
    "                                preprocess=tf.keras.applications.densenet.preprocess_input),\n",
    "            'densenet169': dict(model=tf.keras.applications.densenet.DenseNet169,\n",
    "                                preprocess=tf.keras.applications.densenet.preprocess_input),\n",
    "            'efficientnetb1': dict(model=tf.keras.applications.efficientnet.EfficientNetB1,\n",
    "                                   preprocess=tf.keras.applications.efficientnet.preprocess_input),\n",
    "            'efficientnetb3': dict(model=tf.keras.applications.efficientnet.EfficientNetB3,\n",
    "                                   preprocess=tf.keras.applications.efficientnet.preprocess_input),\n",
    "            'efficientnetb5': dict(model=tf.keras.applications.efficientnet.EfficientNetB5,\n",
    "                                   preprocess=tf.keras.applications.efficientnet.preprocess_input),\n",
    "            'resnet50v2': dict(model=tf.keras.applications.ResNet50V2,\n",
    "                               preprocess=tf.keras.applications.resnet_v2.preprocess_input),\n",
    "            'inceptionv3': dict(model=tf.keras.applications.inception_v3.InceptionV3,\n",
    "                                preprocess=tf.keras.applications.inception_v3.preprocess_input)\n",
    "            }\n",
    "\n",
    "    base_model = arch[model_name]['model'](include_top=False, weights=weights)\n",
    "    preprocess_input = arch[model_name]['preprocess']\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "\n",
    "    # Image augmentation block\n",
    "    if do_augmentation:\n",
    "        x = data_augmentation(inputs)\n",
    "    else:\n",
    "        x = inputs\n",
    "    x = layers.Lambda(lambda x: preprocess_input(x))(x)\n",
    "    x = base_model(x, training=bn_train)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "   \n",
    "    if mode == 'binary':\n",
    "        activation = \"sigmoid\"\n",
    "        units = 1\n",
    "    elif mode == 'categorical':\n",
    "        activation = \"softmax\"\n",
    "        units = len(labels)\n",
    "    elif mode == 'multi-binary':\n",
    "        activation = \"sigmoid\"\n",
    "        units = len(labels)\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "  \n",
    "    outputs = layers.Dense(units, activation=activation)(x)\n",
    "\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    return model, base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_callbacks(path):\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        min_delta=0,\n",
    "        patience=6,\n",
    "        verbose=True,\n",
    "        mode=\"auto\",\n",
    "        restore_best_weights=True,\n",
    "    )\n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                                                     patience=1, min_delta=0.0001, min_lr=1e-6, verbose=True)\n",
    "    csv_logger = tf.keras.callbacks.CSVLogger(filename=f'{path}/log.csv')\n",
    "    # tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=f'{path}/tboard_log')\n",
    "    # checkpoint = tf.keras.callbacks.ModelCheckpoint(f'{path}/checkpoints', save_best_only=True)\n",
    "    return [early_stop, reduce_lr, csv_logger]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(labels=None, mode='binary'):\n",
    "    metrics = ['accuracy']\n",
    "    if mode == 'categorical' and len(labels) > 2:\n",
    "        for i, label in enumerate(labels):\n",
    "            metrics += [MulticlassAUC(pos_label=i, sparse=True,\n",
    "                                      name=f\"AUC_{label.replace(' ', '_')}\")]\n",
    "    else:\n",
    "        multi_label = mode == 'multi-binary'\n",
    "        metrics.append(tf.keras.metrics.AUC(multi_label=multi_label))\n",
    "    if mode == 'multi-binary':\n",
    "        for i, label in enumerate(labels):\n",
    "            class_id = i\n",
    "            metrics += [MulticlassAUC(pos_label=i,\n",
    "                                      name=f\"AUC_{label.replace(' ', '_')}\")]\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bn_gamma_bias_trainable(base_model, momentum=0.99):\n",
    "    base_model.trainable = True\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "        if isinstance(layer, keras.layers.BatchNormalization):\n",
    "            layer.trainable = True\n",
    "            layer.momentum = momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bn_moving_stats_trainable_only(base_model, momentum=0.99):\n",
    "    base_model.trainable = True\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "        if isinstance(layer, keras.layers.BatchNormalization):\n",
    "            layer.trainable = True\n",
    "            layer.momentum = momentum\n",
    "            layer._non_trainable_weights += layer._trainable_weights\n",
    "            layer._trainable_weights = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_and_save_log(path, model, test_ds, history, callbacks,\n",
    "                          method, name='run_log'):\n",
    "    def convert(o):\n",
    "        if isinstance(o, np.float32): return float(o)\n",
    "    logs = {'method': method}\n",
    "    if history:\n",
    "        logs.update({'train': history.history})\n",
    "        logs['best_epoch'] = int(np.argmin(history.history['val_loss']) + 1)\n",
    "    if callbacks:\n",
    "        logs['stopped_epoch'] = callbacks[0].stopped_epoch\n",
    "    log_path = path / f'{name}.json'\n",
    "    if isinstance(test_ds, dict):\n",
    "        for name, ds in test_ds.items():\n",
    "            result = model.evaluate(ds)\n",
    "            logs[f'test_{name}'] = dict(zip(model.metrics_names, result))\n",
    "    else:\n",
    "        result = model.evaluate(test_ds)\n",
    "        logs['test'] = dict(zip(model.metrics_names, result))\n",
    "    with log_path.open('w') as f:\n",
    "        json.dump(logs, f, default=convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_params_to_json(params, path, name='params'):\n",
    "    data = {k: v for k, v in params.__dict__.items() if not k.startswith('__') and k not in {'loss'}}\n",
    "    param_path = path / f'{name}.json'\n",
    "    with param_path.open('w') as f:\n",
    "        json.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_results():\n",
    "    errs = []\n",
    "    rows = []\n",
    "    for f in Path(f'{OUTPUT_PATH}/results/').rglob('*log*.json'):\n",
    "        method = f.parent.parent.parent.stem\n",
    "        model_name = f.parent.parent.parent.parent.stem\n",
    "        dataset_name = f.parent.parent.parent.parent.parent.stem\n",
    "        with f.open() as c:\n",
    "            log_data = json.load(c)\n",
    "        try:\n",
    "            del log_data['train']\n",
    "        except:\n",
    "            errs.append(f)\n",
    "        try:\n",
    "            if 'test_hosp_0' in log_data:\n",
    "                log_data['test'] = {}\n",
    "                for test_hosp in ['test_hosp_0', 'test_hosp_2', 'test_hosp_3']:\n",
    "                    for k in log_data[test_hosp].keys():\n",
    "                        log_data['test'][f\"{k.replace('_1', '')}_{test_hosp}\"] = log_data[test_hosp][k]\n",
    "        except:\n",
    "            pass\n",
    "        for metric in ['auc', 'accuracy', 'loss']:\n",
    "            s, n = 0, 0\n",
    "            for k, v in log_data['test'].items():\n",
    "                if k.lower().startswith(metric):\n",
    "                    s += v\n",
    "                    n += 1\n",
    "            if n > 0:\n",
    "                log_data['test'][metric] = s / n\n",
    "        for met in ['auc_1', 'TPR', 'TNR']:\n",
    "            try:\n",
    "                del log_data['test'][met]\n",
    "            except:\n",
    "                pass\n",
    "        log_data.update(log_data['test'])\n",
    "        del log_data['test']\n",
    "        if f.stem == 'run_log_fc':\n",
    "            log_data['method'] = 'fc'\n",
    "        log_data['model'] = model_name\n",
    "        log_data['dataset'] = dataset_name\n",
    "        log_data['path'] = f\n",
    "        try:\n",
    "            rows.append(log_data)\n",
    "        except:\n",
    "            pass\n",
    "    df = pd.DataFrame(rows)\n",
    "    try:\n",
    "        df_g = df.groupby(['dataset',\n",
    "                           'model',\n",
    "                           'method']).agg(['mean', 'std', 'count']).round(3)\n",
    "    except:\n",
    "        df_g = None\n",
    "    return df, df_g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BN gamma and bias only + last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_bn_and_fc(expParams):\n",
    "    method = expParams.__dict__.get('method', 'bn_and_fc')\n",
    "    tf.keras.backend.clear_session()\n",
    "    datenow = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    path = Path(f'{OUTPUT_PATH}/results/{expParams.dataset_name}/{expParams.model_name}/{method}/runs/{datenow}')\n",
    "    print(path)\n",
    "    path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    train_ds, val_ds, test_ds, params = load_dataset(expParams.dataset_name)\n",
    "    \n",
    "    save_params_to_json(params, path, name='params')\n",
    "\n",
    "    model, base_model = build_model(params.input_shape,\n",
    "                                    model_name=expParams.model_name,\n",
    "                                    bn_train=False, dropout=params.dropout,\n",
    "                                    mode=params.mode,\n",
    "                                    do_augmentation=expParams.do_augmentation,\n",
    "                                    labels=params.labels)\n",
    "    make_bn_gamma_bias_trainable(base_model)\n",
    "    model.compile(loss=params.loss,\n",
    "                  optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "                  metrics=get_metrics(params.labels, mode=params.mode))\n",
    "    callbacks = get_callbacks(str(path))\n",
    "    print(model.summary())\n",
    "    history = model.fit(\n",
    "        train_ds, epochs=MAX_EPOCHS,\n",
    "        initial_epoch=0,\n",
    "        validation_data=val_ds,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "    if expParams.save_model:\n",
    "        model.save(f'{path}/checkpoint')\n",
    "    evaluate_and_save_log(path, model, test_ds, history, callbacks, method=method, name='run_log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FC then BN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_fc_then_bn(expParams, bn_training=False, fc_epochs=None, learning_rate_2=1e-3):\n",
    "    method = expParams.__dict__.get('method', 'fc_then_bn')\n",
    "    tf.keras.backend.clear_session()\n",
    "    datenow = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    path = Path(f'{OUTPUT_PATH}/results/{expParams.dataset_name}/{expParams.model_name}/{method}/runs/{datenow}')\n",
    "    print(path)\n",
    "    path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    train_ds, val_ds, test_ds, params = load_dataset(expParams.dataset_name)\n",
    "\n",
    "    save_params_to_json(params, path, name='params')\n",
    "    \n",
    "    model, base_model = build_model(params.input_shape,\n",
    "                                    model_name=expParams.model_name,\n",
    "                                    bn_train=bn_training, dropout=params.dropout,\n",
    "                                    mode=params.mode,\n",
    "                                    do_augmentation=expParams.do_augmentation,\n",
    "                                    labels=params.labels)\n",
    "\n",
    "    # Train FC first\n",
    "    ## freeze base\n",
    "    base_model.trainable = False\n",
    "    model.compile(loss=params.loss,\n",
    "                  optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "                  metrics=get_metrics(params.labels, mode=params.mode))\n",
    "    callbacks = get_callbacks(str(path))\n",
    "    print(model.summary())\n",
    "    history = model.fit(\n",
    "        train_ds, epochs=100 if fc_epochs is None else fc_epochs,\n",
    "        initial_epoch=0,\n",
    "        validation_data=val_ds,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "    if fc_epochs is None:\n",
    "        stopped_epoch = callbacks[0].stopped_epoch\n",
    "    else:\n",
    "        stopped_epoch = fc_epochs\n",
    "    base_model.trainable = True\n",
    "    make_bn_gamma_bias_trainable(base_model)\n",
    "    model.compile(loss=params.loss,\n",
    "                  optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate_2),\n",
    "                  metrics=get_metrics(params.labels, mode=params.mode))\n",
    "    callbacks = get_callbacks(str(path))\n",
    "    print(model.summary())\n",
    "    history = model.fit(\n",
    "        train_ds, epochs=MAX_EPOCHS,\n",
    "        initial_epoch=stopped_epoch,\n",
    "        validation_data=val_ds,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "    if expParams.save_model:\n",
    "        model.save(f'{path}/checkpoint')\n",
    "    evaluate_and_save_log(path, model, test_ds, history, callbacks, method=method, name='run_log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FC, then full base with lower lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_fc_then_full(expParams, bn_training=False):\n",
    "    method = expParams.__dict__.get('method', 'fc_then_full')\n",
    "    tf.keras.backend.clear_session()\n",
    "    datenow = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    path = Path(f'{OUTPUT_PATH}/results/{expParams.dataset_name}/{expParams.model_name}/{method}/runs/{datenow}')\n",
    "\n",
    "    print(path)\n",
    "    path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    train_ds, val_ds, test_ds, params = load_dataset(expParams.dataset_name)\n",
    "    \n",
    "    save_params_to_json(params, path, name='params')\n",
    "    model, base_model = build_model(params.input_shape,\n",
    "                                    model_name=expParams.model_name,\n",
    "                                    bn_train=bn_training, dropout=params.dropout,\n",
    "                                    mode=params.mode,\n",
    "                                    do_augmentation=expParams.do_augmentation,\n",
    "                                    labels=params.labels)\n",
    "\n",
    "    # Train FC first\n",
    "    ## freeze base\n",
    "    base_model.trainable = False\n",
    "    model.compile(loss=params.loss,\n",
    "                  optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "                  metrics=get_metrics(params.labels, mode=params.mode))\n",
    "    callbacks = get_callbacks(str(path))\n",
    "    print(model.summary())\n",
    "    history = model.fit(\n",
    "        train_ds, epochs=100,\n",
    "        initial_epoch=0,\n",
    "        validation_data=val_ds,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "    if expParams.save_model:\n",
    "        model.save(f'{path}/checkpoint_fc')\n",
    "    evaluate_and_save_log(path, model, test_ds, history, callbacks, method='fc',\n",
    "                          name='run_log_fc')\n",
    "    stopped_epoch = callbacks[0].stopped_epoch\n",
    "    # unfreeze rest of model\n",
    "    base_model.trainable = True\n",
    "    model.compile(loss=params.loss,\n",
    "                  optimizer=keras.optimizers.Adam(learning_rate=1e-5),\n",
    "                  metrics=get_metrics(params.labels, mode=params.mode))\n",
    "    callbacks = get_callbacks(str(path))\n",
    "    print(model.summary())\n",
    "    history = model.fit(\n",
    "        train_ds, epochs=MAX_EPOCHS,\n",
    "        initial_epoch=stopped_epoch,\n",
    "        validation_data=val_ds,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "    if expParams.save_model:\n",
    "        model.save(f'{path}/checkpoint_fc_then_full')\n",
    "    evaluate_and_save_log(path, model, test_ds, history,\n",
    "                          callbacks, method=method, name='run_log_fc_then_full')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving BN + FC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_moving_bn_and_fc(expParams):\n",
    "    method = expParams.__dict__.get('method', 'moving_bn_and_fc')\n",
    "    tf.keras.backend.clear_session()\n",
    "    datenow = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    path = Path(f'{OUTPUT_PATH}/results/{expParams.dataset_name}/{expParams.model_name}/{method}/runs/{datenow}')\n",
    "    print(path)\n",
    "    path.mkdir(exist_ok=True, parents=True)\n",
    "    train_ds, val_ds, test_ds, params = load_dataset(expParams.dataset_name)\n",
    "    save_params_to_json(params, path, name='params')\n",
    "    model, base_model = build_model(params.input_shape,\n",
    "                                    model_name=expParams.model_name,\n",
    "                                    bn_train=None, dropout=params.dropout,\n",
    "                                    mode=params.mode,\n",
    "                                    do_augmentation=expParams.do_augmentation,\n",
    "                                    labels=params.labels)\n",
    "    make_bn_moving_stats_trainable_only(base_model, momentum=0.95)\n",
    "    model.compile(loss=params.loss,\n",
    "                  optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "                  metrics=get_metrics(params.labels, mode=params.mode))\n",
    "    callbacks = get_callbacks(str(path))\n",
    "    print(model.summary())\n",
    "    history = model.fit(\n",
    "        train_ds, epochs=MAX_EPOCHS,\n",
    "        initial_epoch=0,\n",
    "        validation_data=val_ds,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "    if expParams.save_model:\n",
    "        model.save(f'{path}/checkpoint')\n",
    "    evaluate_and_save_log(path, model, test_ds, history, callbacks, method=method, name='run_log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_random_weights_bn_and_fc(expParams, bn_train=None):\n",
    "    method = expParams.__dict__.get('method', 'random_weights_bn_and_fc')\n",
    "    tf.keras.backend.clear_session()\n",
    "    datenow = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    path = Path(f'{OUTPUT_PATH}/results/{expParams.dataset_name}/{expParams.model_name}/{method}/runs/{datenow}')\n",
    "    print(path)\n",
    "    path.mkdir(exist_ok=True, parents=True)\n",
    "    train_ds, val_ds, test_ds, params = load_dataset(expParams.dataset_name)\n",
    "    save_params_to_json(params, path, name='params')\n",
    "    model, base_model = build_model(params.input_shape,\n",
    "                                    model_name=expParams.model_name,\n",
    "                                    bn_train=bn_train, dropout=params.dropout,\n",
    "                                    weights=None,\n",
    "                                    mode=params.mode,\n",
    "                                    do_augmentation=expParams.do_augmentation,\n",
    "                                    labels=params.labels)\n",
    "    make_bn_gamma_bias_trainable(base_model, momentum=0.99)\n",
    "    metrics = get_metrics(params.labels, mode=params.mode)\n",
    "    model.compile(loss=params.loss,\n",
    "                  optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "                  metrics=metrics)\n",
    "    callbacks = get_callbacks(str(path))\n",
    "    print(model.summary())\n",
    "    history = model.fit(\n",
    "        train_ds, epochs=MAX_EPOCHS,\n",
    "        initial_epoch=0,\n",
    "        validation_data=val_ds,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "    for layer in base_model.layers:\n",
    "        if isinstance(layer, keras.layers.BatchNormalization):\n",
    "            print(layer.moving_mean)\n",
    "            break\n",
    "    if expParams.save_model:\n",
    "        model.save(f'{path}/checkpoint')\n",
    "    evaluate_and_save_log(path, model, test_ds, history, callbacks, method=method, name='run_log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_run(dataset_name, model_name, method, df_g, max_count=3):\n",
    "    try:\n",
    "        count = df_g.loc[(dataset_name, model_name, method)][('loss', 'count')]\n",
    "        print(count)\n",
    "        if count < max_count:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods_map = {'fc_then_full': 'FC-then-full',\n",
    "               'fc_then_bn': 'FC-then-BN',\n",
    "               'bn_and_fc': 'FC-BN',\n",
    "               'fc_then_bn_lr': 'FC-then-BN-lr',\n",
    "               'fc': 'FC',\n",
    "               'moving_bn_and_fc': 'FC-BMA',\n",
    "               'fc_then_full_bnt': 'FC-then-full-BMA',\n",
    "               'fc_then_bn_bnt': 'FC-then-BN-BMA',\n",
    "               'random_weights_bn_and_fc': 'RND-FC-BN',\n",
    "               'random_weights_bn_training_and_fc': 'RND-FC-BN-BMA'\n",
    "               }\n",
    "models_map = {'densenet121': 'DenseNet121',\n",
    "              'resnet50v2': 'ResNet50v2',\n",
    "              'inceptionv3': 'InceptionV3',\n",
    "              'efficientnetb3': 'EfficientNetB3'}\n",
    "datasets_map = {'chest_xray': 'Chest X-ray',\n",
    "                'chexpert': 'CheXpert',\n",
    "                'malaria': 'Malaria',\n",
    "                'oct_small': 'OCT small',\n",
    "                'colorectal_histology': 'Colorectal Histology',\n",
    "                'patch_camelyon16_small': 'Patch Camelyon 16 small',\n",
    "                'patch_camelyon16': 'Patch Camelyon16',\n",
    "                'patch_camelyon17_small': 'Patch Camelyon 17 small',\n",
    "                'patch_camelyon17': 'Patch Camelyon 17'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method_dict = {\n",
    "    'bn_and_fc': run_bn_and_fc,\n",
    "    'fc_then_bn': partial(run_fc_then_bn, fc_epochs=1),\n",
    "    'fc_then_bn_bnt': partial(run_fc_then_bn, fc_epochs=1, bn_training=None),\n",
    "    'fc_then_bn_lr': partial(run_fc_then_bn, learning_rate_2=1e-5),\n",
    "    'fc_then_full': run_fc_then_full,\n",
    "    'fc_then_full_bnt': partial(run_fc_then_full, bn_training=None),\n",
    "    'moving_bn_and_fc': run_moving_bn_and_fc,\n",
    "    'random_weights_bn_and_fc': partial(run_random_weights_bn_and_fc, bn_train=False),\n",
    "    'random_weights_bn_training_and_fc': partial(run_random_weights_bn_and_fc, bn_train=None)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, df_g = load_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run_id in range(REPETITIONS):\n",
    "    for method, func in method_dict.items():\n",
    "        for model_name in models_map.keys():\n",
    "            for dataset_name in datasets_map.keys():\n",
    "                class expParams:\n",
    "                    dataset_name = dataset_name\n",
    "                    model_name = model_name\n",
    "                    save_model = False\n",
    "                    do_augmentation = True\n",
    "                    method = method\n",
    "\n",
    "                if should_run(dataset_name, model_name, method, df_g max_count=REPETITIONS):\n",
    "                    print(dataset_name, model_name, method)\n",
    "                    try:\n",
    "                        func(expParams)\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "                    df, df_g = load_results()\n",
    "                else:\n",
    "                    print('already exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_results():\n",
    "    df = pd.read_csv('data/results.csv')\n",
    "    df_g = df.groupby(['dataset', 'model', 'method']).agg(['mean', 'std', 'count']).round(3)\n",
    "    return df, df_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, df_g = load_results()\n",
    "df_g = df_g[\n",
    "    [('loss', 'mean'),\n",
    "     ('loss', 'std'),\n",
    "     ('loss', 'count'),\n",
    "     ('auc', 'mean'),\n",
    "     ('auc', 'std'),\n",
    "     ]\n",
    "]\n",
    "df_g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, df_g = load_results()\n",
    "df = df.replace(datasets_map).replace(models_map).replace(methods_map)\n",
    "\n",
    "methods = [methods_map[m] for m in ['fc_then_full', 'fc_then_bn', 'fc', 'moving_bn_and_fc', 'random_weights_bn_and_fc']]\n",
    "models = ['DenseNet121']\n",
    "datasets = [datasets_map[m] for m in ['chexpert']]\n",
    "df = df[df['method'].isin(methods)]\n",
    "df = df[df['model'].isin(models)]\n",
    "df = df[df['dataset'].isin(datasets)]\n",
    "print(df.columns)\n",
    "df = df[['method', 'AUC_Atelectasis',\n",
    "         'AUC_Cardiomegaly', 'AUC_Consolidation', 'AUC_Edema',\n",
    "         'AUC_Pleural_Effusion']]\n",
    "\n",
    "mean = (df.groupby(['method']).agg(['mean']) * 100).round(1).astype('str')\n",
    "mean.columns = mean.columns.droplevel(1)\n",
    "std = (df.groupby(['method']).agg(['std']) * 100).round(1).astype('str')\n",
    "std.columns = std.columns.droplevel(1)\n",
    "for c in std.columns:\n",
    "    std[c] = std[c].apply(lambda x: f' ({x})')\n",
    "df = mean + std\n",
    "df.columns = ['Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Pleural Effusion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, df_g = load_results()\n",
    "df = df.replace(datasets_map).replace(models_map).replace(methods_map)\n",
    "\n",
    "methods = [methods_map[m] for m in ['fc_then_full', 'fc_then_bn', 'fc', 'moving_bn_and_fc', 'random_weights_bn_and_fc']]\n",
    "models = ['DenseNet121']\n",
    "datasets = [datasets_map[m] for m in ['oct_small']]\n",
    "df = df[df['method'].isin(methods)]\n",
    "df = df[df['model'].isin(models)]\n",
    "df = df[df['dataset'].isin(datasets)]\n",
    "print(df.columns)\n",
    "df = df[['method', 'AUC_0',\n",
    "         'AUC_1', 'AUC_2', 'AUC_3']]\n",
    "\n",
    "mean = (df.groupby(['method']).agg(['mean']) * 100).round(1).astype('str')\n",
    "mean.columns = mean.columns.droplevel(1)\n",
    "std = (df.groupby(['method']).agg(['std']) * 100).round(1).astype('str')\n",
    "std.columns = std.columns.droplevel(1)\n",
    "for c in std.columns:\n",
    "    std[c] = std[c].apply(lambda x: f' ({x})')\n",
    "df = mean + std\n",
    "df.columns = ['CNV', 'DME', 'DRUSEN', 'NORMAL']\n",
    "\n",
    "print(df.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, df_g = load_results()\n",
    "df = df.replace(datasets_map).replace(models_map).replace(methods_map)\n",
    "\n",
    "methods = [methods_map[m] for m in ['fc_then_full', 'fc_then_bn', 'fc', 'moving_bn_and_fc', 'random_weights_bn_and_fc']]\n",
    "models = ['DenseNet121']\n",
    "datasets = [datasets_map[m] for m in ['colorectal_histology']]\n",
    "df = df[df['method'].isin(methods)]\n",
    "df = df[df['model'].isin(models)]\n",
    "df = df[df['dataset'].isin(datasets)]\n",
    "print(df.columns)\n",
    "df = df[['method', 'AUC_tumor', 'AUC_stroma', 'AUC_complex', 'AUC_lympho', 'AUC_debris',\n",
    "         'AUC_mucosa', 'AUC_adipose', 'AUC_empty']]\n",
    "\n",
    "mean = (df.groupby(['method']).agg(['mean']) * 100).round(1).astype('str')\n",
    "mean.columns = mean.columns.droplevel(1)\n",
    "std = (df.groupby(['method']).agg(['std']) * 100).round(1).astype('str')\n",
    "std.columns = std.columns.droplevel(1)\n",
    "for c in std.columns:\n",
    "    std[c] = std[c].apply(lambda x: f' ({x})')\n",
    "df = mean + std\n",
    "df.columns = [s.split('_')[1].capitalize() for s in\n",
    "              ['AUC_tumor', 'AUC_stroma', 'AUC_complex', 'AUC_lympho', 'AUC_debris',\n",
    "               'AUC_mucosa', 'AUC_adipose', 'AUC_empty']]\n",
    "\n",
    "print(df.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, df_g = load_results()\n",
    "df = df.replace(datasets_map).replace(models_map).replace(methods_map)\n",
    "\n",
    "methods = [methods_map[m] for m in ['fc_then_full', 'fc_then_bn', 'fc', 'moving_bn_and_fc', 'random_weights_bn_and_fc']]\n",
    "models = ['DenseNet121']\n",
    "datasets = [datasets_map[m] for m in ['patch_camelyon17', 'patch_camelyon17_small']]\n",
    "df = df[df['method'].isin(methods)]\n",
    "df = df[df['model'].isin(models)]\n",
    "df = df[df['dataset'].isin(datasets)]\n",
    "\n",
    "df = df[['method', 'dataset', 'auc_test_hosp_2', 'auc_test_hosp_3', 'auc_test_hosp_0']]\n",
    "\n",
    "mean = (df.groupby(['dataset', 'method']).agg(['mean']) * 100).round(1).astype('str')\n",
    "mean.columns = mean.columns.droplevel(1)\n",
    "std = (df.groupby(['dataset', 'method']).agg(['std']) * 100).round(1).astype('str')\n",
    "std.columns = std.columns.droplevel(1)\n",
    "for c in std.columns:\n",
    "    std[c] = std[c].apply(lambda x: f' ({x})')\n",
    "df = mean + std\n",
    "\n",
    "df.columns = ['Hosp. 0', 'Hosp. 2', 'Hosp. 3']\n",
    "print(df.to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, df_g = load_results()\n",
    "\n",
    "df = df.replace(datasets_map).replace(models_map).replace(methods_map)\n",
    "methods = [methods_map[m] for m in ['fc_then_full', 'fc_then_bn', 'fc', 'moving_bn_and_fc', 'random_weights_bn_and_fc']]\n",
    "models = models_map.values()\n",
    "datasets = datasets_map.values()\n",
    "df = df[df['method'].isin(methods)]\n",
    "sns.set(font_scale=2, rc={'text.usetex': True})\n",
    "\n",
    "g = sns.catplot(x='model', y='auc', hue='method',\n",
    "                hue_order=methods,\n",
    "                order=models,\n",
    "                data=df, col='dataset', col_order=datasets, ci=\"sd\", kind='bar', col_wrap=3,\n",
    "                height=5, aspect=1.5,\n",
    "                legend=False, legend_out=False\n",
    "                )\n",
    "\n",
    "(g.set_axis_labels(\"\", \"AUC\")\n",
    " .set_titles(\"{col_name}\")\n",
    " .set(ylim=(0.45, 1))\n",
    " .despine(left=True).set_xticklabels(rotation=90))\n",
    "\n",
    "handles = g._legend_data.values()\n",
    "labels = g._legend_data.keys()\n",
    "g.fig.legend(handles=handles, labels=labels, loc='upper center', ncol=5)\n",
    "g.fig.subplots_adjust(top=0.92, bottom=0.08)\n",
    "# g.savefig(f'{OUTPUT_PATH}/auc_all.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, df_g = load_results()\n",
    "df = df.replace(datasets_map).replace(models_map).replace(methods_map)\n",
    "\n",
    "methods = [methods_map[m] for m in\n",
    "           ['fc_then_full', 'fc_then_full_bnt', 'fc_then_bn', 'fc_then_bn_bnt', 'fc', 'moving_bn_and_fc']]\n",
    "models = ['DenseNet121']\n",
    "datasets = datasets_map.values()\n",
    "df = df[df['method'].isin(methods)]\n",
    "\n",
    "sns.set(font_scale=2, rc={'text.usetex': True})\n",
    "g = sns.catplot(x='model', y='auc', hue='method',\n",
    "                hue_order=methods,\n",
    "                order=models,\n",
    "                data=df, col='dataset', col_order=datasets, ci=\"sd\", kind='bar', col_wrap=3,\n",
    "                height=6, aspect=1.5, legend=False)\n",
    "\n",
    "(g.set_axis_labels(\"\", \"AUC\")\n",
    " .set_titles(\"{col_name}\")\n",
    " .set(ylim=(0.45, 1))\n",
    " .despine(left=True).set_xticklabels(rotation=90))\n",
    "handles = g._legend_data.values()\n",
    "labels = g._legend_data.keys()\n",
    "g.fig.legend(handles=handles, labels=labels, loc='upper center', ncol=6)\n",
    "g.fig.subplots_adjust(top=0.92, bottom=0.08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, df_g = load_results()\n",
    "df = df.replace(datasets_map).replace(models_map).replace(methods_map)\n",
    "methods = [methods_map[m] for m in ['random_weights_bn_and_fc', 'random_weights_bn_training_and_fc']]\n",
    "models = [models_map[m] for m in ['densenet121', 'efficientnetb3']]\n",
    "datasets = datasets_map.values()\n",
    "df = df[df['method'].isin(methods)]\n",
    "\n",
    "sns.set(font_scale=2)\n",
    "g = sns.catplot(x='model', y='auc', hue='method',\n",
    "                hue_order=methods,\n",
    "                order=models,\n",
    "                data=df, col='dataset', col_order=datasets, ci=\"sd\", kind='bar', col_wrap=3,\n",
    "                height=5, aspect=1.5, legend=False)\n",
    "\n",
    "(g.set_axis_labels(\"\", \"AUC\")\n",
    " .set_titles(\"{col_name}\")\n",
    " .set(ylim=(0.45, 1))\n",
    " .despine(left=True).set_xticklabels(rotation=90))\n",
    "handles = g._legend_data.values()\n",
    "labels = g._legend_data.keys()\n",
    "g.fig.legend(handles=handles, labels=labels, loc='upper center', ncol=6)\n",
    "g.fig.subplots_adjust(top=0.92, bottom=0.08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, df_g = load_results()\n",
    "df = df.replace(datasets_map).replace(models_map).replace(methods_map)\n",
    "methods = [methods_map[m] for m in ['fc_then_bn', 'bn_and_fc', 'fc_then_bn_lr']]\n",
    "models = [models_map[m] for m in ['densenet121', 'efficientnetb3']]\n",
    "datasets = datasets_map.values()\n",
    "df = df[df['method'].isin(methods)]\n",
    "\n",
    "sns.set(font_scale=2)\n",
    "g = sns.catplot(x='model', y='auc', hue='method',\n",
    "                hue_order=methods,\n",
    "                order=models,\n",
    "                data=df, col='dataset', col_order=datasets, ci=\"sd\", kind='bar', col_wrap=3,\n",
    "                height=5, aspect=1.5, legend=False)\n",
    "\n",
    "(g.set_axis_labels(\"\", \"AUC\")\n",
    " .set_titles(\"{col_name}\")\n",
    " .set(ylim=(0.45, 1))\n",
    " .despine(left=True).set_xticklabels(rotation=90))\n",
    "handles = g._legend_data.values()\n",
    "labels = g._legend_data.keys()\n",
    "g.fig.legend(handles=handles, labels=labels, loc='upper center', ncol=6)\n",
    "g.fig.subplots_adjust(top=0.92, bottom=0.08)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
